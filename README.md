# spring-ai-llm-comparison-project
A Spring Boot application project exploring and comparing multiple LLMs (OpenAI, Anthropic, and Ollama) using Spring AI for generative text, response quality, and latency evaluation

üöÄ Project Purpose

Compare behaviour and performance of different LLMs (response quality, latency, cost, etc.) under the same prompts.

Provide an abstraction layer (via Spring Boot + Spring AI) so switching between LLM providers is easier.

Evaluate which LLM works best for specific use cases (e.g. summarization, code generation, chat, embedding, etc.).

Serve as a reference / template for developers to build AI-powered applications with pluggable LLM support.

üß∞ Tech Stack & Tools
Layer / Component	Technology / Purpose
Backend Framework	Spring Boot + Spring AI (for LLM integration) 
Home
+1

Java Version / Build Tool	Java 17+ / Maven or Gradle
LLM Providers	Any REST-compatible LLM backend (e.g. open-source LLMs, API-based LLMs, local or remote)
Testing / Comparison Scripts	Scripts to send same prompt to different LLMs and compare outputs & metrics (latency, token usage, quality)
Configuration	YAML or properties files to define LLM endpoints / keys / model parameters
üìÅ Suggested Project Structure
src/
‚îú‚îÄ‚îÄ config/         # Spring AI / LLM configuration (endpoints, credentials, model settings)
‚îú‚îÄ‚îÄ controller/     # REST controllers or services for sending prompts / receiving responses
‚îú‚îÄ‚îÄ service/        # Business logic: wrapper around ChatClient / LLM calls + response handling
‚îú‚îÄ‚îÄ dto/            # Request / Response data transfer objects
‚îú‚îÄ‚îÄ util/           # Utility classes (prompt templates, evaluation helpers, metrics tracking)
‚îî‚îÄ‚îÄ test/           # Scripts/tests to compare LLM outputs under different conditions

‚öôÔ∏è How to Use / Run Locally

Clone the repository

git clone https://github.com/your-username/spring-ai-llm-comparison.git
cd spring-ai-llm-comparison


Update configuration: e.g. application.yml or .properties file with your LLM endpoints, API keys, model names, etc.

Build and run Spring Boot application

mvn clean install
mvn spring-boot:run


Send prompts via REST endpoint (or using provided test scripts) ‚Äî application will route the request to one (or more) configured LLM backends, return responses, and log metrics (latency, tokens used, response content).

üß™ Comparison / Evaluation Features

Send identical prompt to multiple LLMs and record responses.

Track latency / response time for each model.

Optionally record token usage / cost (if using API-based LLMs).

Allow configurable prompt templates or context injection (e.g. system prompt, user prompt, few-shot examples).

Collect qualitative or quantitative evaluation (e.g. readability, correctness, hallucination, consistency ‚Äî maybe manually rated or via automated metrics).

‚úÖ Potential Use Cases

Testing which LLM works best for code generation, text summarization, Q&A, chatbot, content generation.

Building a fallback mechanism: if one LLM fails / is slow / expensive ‚Äî fallback to another.

Useful as a baseline or benchmarking project when evaluating new LLMs or providers.

Prototyping AI-powered microservices in a unified way.

üìå What to Add / Future Improvements

More LLM providers / engines support (open-source + commercial + self-hosted).

Automated evaluation metrics (e.g. BLEU / ROUGE / embedding-based similarity) for comparing outputs.

Logging & analytics (store response time, success rate, output quality, cost) ‚Äî helps in real-world decision making.

Abstract interface / config so that adding new LLM provider is easy (plug-and-play).

Sample prompt templates (few-shot, zero-shot, system/user roles, context injection).

Documentation on which LLMs were tested, their configurations, and performance benchmark results (maybe in a ‚Äúbenchmarks/‚Äù folder).

Unit & integration tests to ensure the wrapper + evaluation logic works reliably.

Optional: Docker / container setup (if you plan to deploy or share with others).

üìÑ README for Resume / Portfolio (Copy-Paste)

Developed a comparative LLM evaluation backend using Spring Boot and Spring AI. The tool wraps multiple LLM providers behind a unified API, enabling standardized prompt execution and comparison across models. Includes features to measure latency, token usage, and response content for benchmarking. Designed to help choose best-fit LLM for tasks like summarization, chat, code generation, or content creation.
